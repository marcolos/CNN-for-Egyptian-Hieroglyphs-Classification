{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_3hF9LcDNYWC"
   },
   "outputs": [],
   "source": [
    "# ----------- README for the Datasets ----------------\n",
    "#  Dataset_oland ha 4210 immagini e 172 diverse labels con dim 50x75[w,h] in grayscale\n",
    "#  Dataset_nostro ha 1310 immagini e 48 diverse labels con dim 50x75[w,h] in grayscale\n",
    "#  I due dataset hanno in comune 40 label diverse e utilizzando solo queste otteniamo\n",
    "#       - Dataset_oland_adjust ha 3101 immagini con 40 labels diverse\n",
    "#       - Dataset_nostro_adjust ha 1207 immagini con 40 labels diverse\n",
    "#       - Le 8 labels di Dataset_nostro che non possono essere classificate sono: A1,A40,Aa1,D55,U6,W3,Y4,Z2\n",
    "#       => tot_img = 4308 , tot_classes = 40 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "oF5J9vHiMTno",
    "outputId": "06c612d0-b651-44c7-90dc-de6613e4ce8b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "# LOCALE\n",
    "\n",
    "%run util.ipynb\n",
    "%run model.ipynb\n",
    "\n",
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard\n",
    "\n",
    "path_tot = \"/Users/marco/Desktop/tesi/campioni/datasets(200x200)/tot\"\n",
    "path_train = \"/Users/marco/Desktop/tesi/campioni/datasets(200x200)/train\"\n",
    "path_testA = \"/Users/marco/Desktop/tesi/campioni/datasets(200x200)/testA\"\n",
    "path_testB = \"/Users/marco/Desktop/tesi/campioni/datasets(200x200)/testB\"\n",
    "path_train_aug = \"/Users/marco/Desktop/tesi/campioni/datasets(200x200)/train_aug\"\n",
    "\n",
    "path_train_flip = \"/Users/marco/Desktop/tesi/campioni/datasets(200x200)/train_flip\"\n",
    "path_test_flip = \"/Users/marco/Desktop/tesi/campioni/datasets(200x200)/test_flip\"\n",
    "path_train_flip_aug = \"/Users/marco/Desktop/tesi/campioni/datasets(200x200)/train_flip_aug\"\n",
    "path_test_flip_aug = \"/Users/marco/Desktop/tesi/campioni/datasets(200x200)/test_flip_aug\"\n",
    "\n",
    "\n",
    "log_filepath = \"/Users/marco/Desktop/tesi/code/logs\"\n",
    "checkpoint_filepath = \"/Users/marco/Desktop/tesi/code/weights\"\n",
    "weights_path = \"/Users/marco/Desktop/tesi/risultati/my_Xception2/weights/no_aug_accuracy_ADAM\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 41642,
     "status": "ok",
     "timestamp": 1615302231097,
     "user": {
      "displayName": "Marco Loschiavo",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi9s3_7KCzffUmLttlgv1bC9AxkHEpGTlOL7vWRvA=s64",
      "userId": "16918725666776522759"
     },
     "user_tz": -60
    },
    "id": "YPvrUATWNjlL",
    "outputId": "2bf56644-b92f-4604-9739-937e75eb9af0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "# GOOGLE COOLAB\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "!unzip -qu \"/content/drive/MyDrive/ColabNotebooks/tot.zip\" -d \"/content\" \n",
    "!unzip -qu \"/content/drive/MyDrive/ColabNotebooks/train.zip\" -d \"/content\" # -q quit mode\n",
    "!unzip -qu \"/content/drive/MyDrive/ColabNotebooks/testA.zip\" -d \"/content\" # -u part controls extraction only if new/necessary\n",
    "!unzip -qu \"/content/drive/MyDrive/ColabNotebooks/testB.zip\" -d \"/content\" # -d creates the directory and extracted files are stored there.\n",
    "!unzip -qu \"/content/drive/MyDrive/ColabNotebooks/train_aug.zip\" -d \"/content\" \n",
    "!unzip -qu \"/content/drive/MyDrive/ColabNotebooks/train_flip.zip\" -d \"/content\" \n",
    "!unzip -qu \"/content/drive/MyDrive/ColabNotebooks/test_flip.zip\" -d \"/content\" \n",
    "!unzip -qu \"/content/drive/MyDrive/ColabNotebooks/train_flip_aug.zip\" -d \"/content\" \n",
    "!unzip -qu \"/content/drive/MyDrive/ColabNotebooks/test_flip_aug.zip\" -d \"/content\" \n",
    "\n",
    "%run /content/drive/MyDrive/ColabNotebooks/code/util.ipynb\n",
    "%run /content/drive/MyDrive/ColabNotebooks/code/model.ipynb\n",
    "\n",
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard\n",
    "\n",
    "# PATH\n",
    "path_tot = \"/content/tot\"\n",
    "path_train = \"/content/train\"\n",
    "path_testA = '/content/testA'\n",
    "path_testB = '/content/testB'\n",
    "path_train_aug = \"/content/train_aug\"\n",
    "\n",
    "path_train_flip = \"/content/train_flip\"\n",
    "path_test_flip = \"/content/test_flip\"\n",
    "path_train_flip_aug = \"/content/train_flip_aug\"\n",
    "path_test_flip_aug = \"/content/test_flip_aug\"\n",
    "\n",
    "\n",
    "log_filepath = \"/content/drive/MyDrive/ColabNotebooks/logs\"\n",
    "checkpoint_filepath = \"/content/drive/MyDrive/ColabNotebooks/weights\"\n",
    "weights_path = \"/content/drive/MyDrive/ColabNotebooks/weights\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BjNnMksPoH_8"
   },
   "outputs": [],
   "source": [
    "# INITIAL JOIN DATASET(OLAND+OURS) E SALVATAGGIO DEL TRAIN E DEL TEST\n",
    "\n",
    "def initial_join_dataset():\n",
    "  \n",
    "    # dataset path\n",
    "    path_oland = \"/Users/marco/Desktop/tesi/campioni/Oland_campioni(200x200)\"\n",
    "    path = '/Users/marco/Desktop/tesi/campioni/CampioniL_V3(200x200)'\n",
    "\n",
    "    # creo i dataset\n",
    "    dataset_images_oland, dataset_labels_oland = load_dataset(path_oland, \"png\")\n",
    "    dataset_images, dataset_labels = load_dataset(path, \"jpg\")\n",
    "\n",
    "    # tolgo dal dataset nostro le immagini con etichette che non sono presenti nel dataset degli olandese\n",
    "    dataset_images, dataset_labels = adjust_dataset1_to_dataset2(dataset_images, dataset_labels,\n",
    "                                                              dataset_images_oland, dataset_labels_oland)\n",
    "\n",
    "    # tolgo dal dataset olandese tutte le immagini che non sono presenti negli olandesi(addestro solo per quelli che mi servono)\n",
    "    dataset_images_oland, dataset_labels_oland = adjust_dataset1_to_dataset2(dataset_images_oland, dataset_labels_oland,\n",
    "                                                              dataset_images, dataset_labels)\n",
    "\n",
    "\n",
    "    # encoding delle label  # https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html\n",
    "    label_enc = preprocessing.LabelEncoder()\n",
    "    label_enc.fit(dataset_labels_oland)  \n",
    "    dataset_labels_oland_encoded = label_enc.transform(dataset_labels_oland) # trasforma le label da caratteri a numeri\n",
    "    dataset_labels_encoded = label_enc.transform(dataset_labels) # trasforma le label da caratteri a numeri\n",
    "    n_classes = len(list(label_enc.classes_))\n",
    "\n",
    "    # trasformo da list a np array\n",
    "    dataset_images_oland = np.asarray(dataset_images_oland)\n",
    "    dataset_images = np.asarray(dataset_images)\n",
    "\n",
    "    # Aggiungo una extra-dimensione alle immagini e hot-encoding dei labels perche' e' come le vuole in input il modello cnn\n",
    "    dataset_images_oland, dataset_labels_oland_encoded = add_extra_dim(dataset_images_oland, dataset_labels_oland_encoded, n_classes)\n",
    "    dataset_images, dataset_labels_encoded = add_extra_dim(dataset_images, dataset_labels_encoded, n_classes)\n",
    "\n",
    "    print('Dimensione array di immagini oland dopo il reshape:',np.shape(dataset_images_oland))\n",
    "    print('Dimensione array labels oland:',np.shape(dataset_labels_oland_encoded))\n",
    "    print('Dimensione array di immagini dopo il reshape:',np.shape(dataset_images))\n",
    "    print('Dimensione array labels:',np.shape(dataset_labels_encoded))\n",
    "\n",
    "\n",
    "    # SCELTA DEL TRAIN E DEL TEST\n",
    "\n",
    "    # Splitting del dataset in train and test\n",
    "    X_train_A, X_test_A, y_train_A, y_test_A = train_test_split(dataset_images_oland, dataset_labels_oland_encoded, test_size=0.15, stratify=dataset_labels_oland_encoded, random_state=123)\n",
    "    X_train_B, X_test_B, y_train_B, y_test_B = train_test_split(dataset_images, dataset_labels_encoded, test_size=0.15, stratify=dataset_labels_encoded, random_state=123)\n",
    "\n",
    "    # Unione dataset\n",
    "    X_train = np.append(X_train_A, X_train_B, axis= 0)\n",
    "    y_train = np.append(y_train_A, y_train_B, axis= 0)\n",
    "    X_test = np.append(X_test_A, X_test_B, axis= 0)\n",
    "    y_test = np.append(y_test_A, y_test_B, axis= 0)\n",
    "    X_tot = np.append(X_train, X_test, axis= 0)\n",
    "    y_tot = np.append(y_train, y_test, axis= 0)\n",
    "    \n",
    "\n",
    "    # SCELTA DEL TRAIN E DEL TEST CON DIVISIONE ANCHE DEL VALIDATION\n",
    "    #X_train_A, X_val_test_A, y_train_A, y_val_test_A = train_test_split(dataset_images_oland, dataset_labels_oland_encoded, test_size=0.3, random_state=123)\n",
    "    #X_train_B, X_val_test_B, y_train_B, y_val_test_B = train_test_split(dataset_images, dataset_labels_encoded, test_size=0.3, random_state=123)\n",
    "    #X_val_A, X_test_A, y_val_A, y_test_A = train_test_split(X_val_test_A, y_val_test_A, test_size=0.5, random_state=123)\n",
    "    #X_val_B, X_test_B, y_val_B, y_test_B = train_test_split(X_val_test_B, y_val_test_B, test_size=0.5, random_state=123)\n",
    "    # Unione dataset\n",
    "    #X_train = np.append(X_train_A, X_train_B, axis= 0)\n",
    "    #y_train = np.append(y_train_A, y_train_B, axis= 0)\n",
    "    #print('Train images and labels: ',X_train.shape, y_train.shape)\n",
    "    #print('Val images and labels of oland dataset: ',X_val_A.shape, y_val_A.shape)\n",
    "    #print('Val images and labels of ours dataset: ',X_val_B.shape, y_val_B.shape)\n",
    "    #print('Test images and labels of oland dataset: ',X_test_A.shape, y_test_A.shape)\n",
    "    #print('Test images and labels of ours dataset: ',X_test_B.shape, y_test_B.shape)\n",
    "\n",
    "\n",
    "    #SAVE DATASETS\n",
    "    #save_dataset(\"/Users/marco/Desktop/trainA\", X_train_A, categorical_to_decoded(y_train_A, label_enc))\n",
    "    #save_dataset(\"/Users/marco/Desktop/trainB\", X_train_B, categorical_to_decoded(y_train_B, label_enc))\n",
    "    #save_dataset(\"/Users/marco/Desktop/testA\", X_test_A, categorical_to_decoded(y_test_A, label_enc))\n",
    "    #save_dataset(\"/Users/marco/Desktop/testB\", X_test_B, categorical_to_decoded(y_test_B, label_enc))\n",
    "    #save_dataset(\"/Users/marco/Desktop/train\", X_train, categorical_to_decoded(y_train, label_enc))\n",
    "    #save_dataset(\"/Users/marco/Desktop/test\", X_test, categorical_to_decoded(y_test, label_enc))\n",
    "    #save_dataset(\"/Users/marco/Desktop/tot\", X_tot, categorical_to_decoded(y_tot, label_enc))\n",
    "\n",
    "create_and_save_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "CdVz5sVbNYWF",
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "load_dataset() got an unexpected keyword argument 'p'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-06b476580fc0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# loading datasets from disco\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mX_tot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_tot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_tot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"jpg\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"jpg\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mX_test_A\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test_A\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_testA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"jpg\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mX_test_B\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test_B\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_testB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"jpg\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: load_dataset() got an unexpected keyword argument 'p'"
     ]
    }
   ],
   "source": [
    "# DATASET PREPARATION\n",
    "     \n",
    "\n",
    "# loading datasets from disco\n",
    "X_tot, y_tot = load_dataset(path_tot, \"jpg\")\n",
    "X_train, y_train = load_dataset(path_train, \"jpg\")\n",
    "X_test_A, y_test_A = load_dataset(path_testA, \"jpg\")\n",
    "X_test_B, y_test_B = load_dataset(path_testB, \"jpg\")\n",
    "X_train_aug, y_train_aug = load_dataset(path_train_aug, \"jpg\")\n",
    "#X_train_flip, y_train_flip = load_dataset(path_train_flip, \"jpg\")\n",
    "#X_test_flip, y_test_flip = load_dataset(path_test_flip, \"jpg\")\n",
    "#X_train_flip_aug, y_train_flip_aug = load_dataset(path_train_flip_aug, \"jpg\")\n",
    "#X_test_flip_aug, y_test_flip_aug = load_dataset(path_test_flip_aug, \"jpg\")\n",
    "\n",
    "# encoding delle label  # https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html\n",
    "label_enc = preprocessing.LabelEncoder()\n",
    "label_enc.fit(y_tot) \n",
    "y_tot = label_enc.transform(y_tot) # trasforma le label da caratteri a numeri\n",
    "y_train = label_enc.transform(y_train) # trasforma le label da caratteri a numeri\n",
    "y_test_A = label_enc.transform(y_test_A) # trasforma le label da caratteri a numeri\n",
    "y_test_B = label_enc.transform(y_test_B) # trasforma le label da caratteri a numeri\n",
    "y_train_aug = label_enc.transform(y_train_aug) # trasforma le label da caratteri a numeri\n",
    "#y_train_flip = label_enc.transform(y_train_flip) # trasforma le label da caratteri a numeri\n",
    "#y_test_flip = label_enc.transform(y_test_flip) # trasforma le label da caratteri a numeri\n",
    "#y_train_flip_aug = label_enc.transform(y_train_flip_aug) # trasforma le label da caratteri a numeri\n",
    "#y_test_flip_aug = label_enc.transform(y_test_flip_aug) # trasforma le label da caratteri a numeri\n",
    "\n",
    "n_classes = len(list(label_enc.classes_))\n",
    "\n",
    "# trasformo da list a np array\n",
    "X_tot = np.asarray(X_tot)\n",
    "X_train = np.asarray(X_train)\n",
    "X_test_A = np.asarray(X_test_A)\n",
    "X_test_B = np.asarray(X_test_B)\n",
    "X_train_aug = np.asarray(X_train_aug)\n",
    "#X_train_flip = np.asarray(X_train_flip)\n",
    "#X_test_flip = np.asarray(X_test_flip)\n",
    "#X_train_flip_aug = np.asarray(X_train_flip_aug)\n",
    "#X_test_flip_aug = np.asarray(X_test_flip_aug)\n",
    "\n",
    "\n",
    "# Aggiungo una extra-dimensione alle immagini e faccio Hot-encoding dei labels perche' e' come le vuole in input il modello cnn\n",
    "X_tot, y_tot = add_extra_dim(X_tot, y_tot, n_classes)\n",
    "X_train, y_train = add_extra_dim(X_train, y_train, n_classes)\n",
    "X_test_A, y_test_A = add_extra_dim(X_test_A, y_test_A, n_classes)\n",
    "X_test_B, y_test_B = add_extra_dim(X_test_B, y_test_B, n_classes)\n",
    "X_train_aug, y_train_aug = add_extra_dim(X_train_aug, y_train_aug, n_classes)\n",
    "#X_train_flip, y_train_flip = add_extra_dim(X_train_flip, y_train_flip, n_classes)\n",
    "#X_test_flip, y_test_flip = add_extra_dim(X_test_flip, y_test_flip, n_classes)\n",
    "#X_train_flip_aug, y_train_flip_aug = add_extra_dim(X_train_flip_aug, y_train_flip_aug, n_classes)\n",
    "#X_test_flip_aug, y_test_flip_aug = add_extra_dim(X_test_flip_aug, y_test_flip_aug, n_classes)\n",
    "\n",
    "print('Tot images and labels of both datasets: ',X_tot.shape, y_tot.shape)\n",
    "print('Train images and labels: ',X_train.shape, y_train.shape)\n",
    "print('Test A images and labels of oland dataset: ',X_test_A.shape, y_test_A.shape)\n",
    "print('Test B images and labels of ours dataset: ',X_test_B.shape, y_test_B.shape)\n",
    "print('Train images and labels with augmentation: ',X_train_aug.shape, y_train_aug.shape)\n",
    "#print('Train flip images and labels: ',X_train_flip.shape, y_train_flip.shape)\n",
    "#print('Test flip images and labels: ',X_test_flip.shape, y_test_flip.shape)\n",
    "#print('Train flip with aug images and labels: ',X_train_flip_aug.shape, y_train_flip_aug.shape)\n",
    "#print('Test flip with aug images and labels: ',X_test_flip_aug.shape, y_test_flip_aug.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1215,
     "status": "ok",
     "timestamp": 1615312106203,
     "user": {
      "displayName": "Marco Loschiavo",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi9s3_7KCzffUmLttlgv1bC9AxkHEpGTlOL7vWRvA=s64",
      "userId": "16918725666776522759"
     },
     "user_tz": -60
    },
    "id": "m8UMMDbb5HPE",
    "outputId": "3f5fc7aa-539f-4e2b-d1e4-51b9a4595b4b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset created\n"
     ]
    }
   ],
   "source": [
    "#save_dataset(\"/Users/marco/Desktop/train_flip\", X_train, categorical_to_decoded(y_train, label_enc), horizontal_flip=True)\n",
    "save_dataset(\"/Users/marco/Desktop/test_flip\", np.append(X_test_A, X_test_B, axis= 0), categorical_to_decoded(np.append(y_test_A, y_test_B, axis= 0), label_enc), horizontal_flip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h7Ksc0SbhDID",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#train_test_labels_analysis(y_test_A, y_test_B, label_enc)\n",
    "#get_labels_number_in_category(dataset_labels_aug, label_enc, view=True)\n",
    "#get_labels_number_in_category(y_test, label_enc, view=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SpMtW3QsEJ3e"
   },
   "source": [
    "# Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PPhXcI9zEqIG"
   },
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    rotation_range=20, # rotation\n",
    "    width_shift_range=0.1, # horizontal shift\n",
    "    height_shift_range=0.1, # vertical shift\n",
    "    zoom_range=0.05, # zoom\n",
    "    horizontal_flip=True, # horizontal flip\n",
    "    brightness_range=[0.2,1.2]) # brightness\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    directory='/Users/marco/Desktop/prova',\n",
    "    color_mode=\"grayscale\",\n",
    "    batch_size=1\n",
    ")\n",
    "\n",
    "for i in range(0, 5):\n",
    "    next_it = next(train_generator)\n",
    "    image = next_it[0]\n",
    "    label = next_it[1]\n",
    "    #images = np.append(images, image, axis= 0)\n",
    "    #labels = np.append(labels, label, axis=0)\n",
    "    #print(\"augmentation of \" + str(i+1) + \" data\")\n",
    "    plt.imshow(image[0], cmap=\"gray\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZyYfvifONYWI",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#DATA AUGMENTATION\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=20, # rotation\n",
    "    width_shift_range=0.1, # horizontal shift\n",
    "    height_shift_range=0.1, # vertical shift\n",
    "    zoom_range=0.05, # zoom\n",
    "    horizontal_flip=True, # horizontal flip\n",
    "    brightness_range=[0.2,1.2]) # brightness\n",
    "#datagen.fit(X_train)\n",
    "\n",
    "n_aug = 10000 - np.shape(X_train)[0] # in modo da arrivare a 1000 campioni di train\n",
    "X_train_aug, y_train_aug = data_augmentation(datagen, X_train, y_train, n_aug)\n",
    "\n",
    "\n",
    "save_dataset(path_to_save_aug, X_train_aug, categorical_to_decoded(y_train_aug, label_enc))\n",
    "\n",
    "print('Dimensione array di immagini oland dopo data aug:',np.shape(X_train_aug))\n",
    "print('Dimensione array labels oland dopo il data aug:',np.shape(y_train_aug))  \n",
    "\n",
    "#for i in range(1,n_aug):\n",
    "#    label_int = np.argmax(y_train[3100+i]) \n",
    "#    label = label_enc.inverse_transform([label_int])[0]   \n",
    "#    plt.figure(i)\n",
    "#    plt.imshow(X_train[3100+i], cmap=\"gray\")\n",
    "#    plt.title(label)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GmP2-WeUoIAA"
   },
   "outputs": [],
   "source": [
    "# CREATE AND SAVE DATASET(train(with_aug))\n",
    "\n",
    "def create_and_save_data_augmentation_dataset(save=False):\n",
    "    # dataset path\n",
    "    path_oland = \"/Users/marco/Desktop/datasets/trainA\"\n",
    "    path = '/Users/marco/Desktop/datasets/trainB'\n",
    "\n",
    "    # creo i filelist(path di tutte le immagini del dataset) dei dataset\n",
    "    filelist_oland = create_filelist(path_oland, \"jpg\") \n",
    "    filelist = create_filelist(path, \"jpg\")       \n",
    "\n",
    "    # creo i dataset\n",
    "    dataset_images_oland, dataset_labels_oland = make_dataset(filelist_oland)\n",
    "    dataset_images, dataset_labels = make_dataset(filelist)\n",
    "\n",
    "    \n",
    "    dataset_images = dataset_images_oland + dataset_images\n",
    "    dataset_labels = dataset_labels_oland + dataset_labels\n",
    "\n",
    "\n",
    "    # encoding delle label  # https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html\n",
    "    label_enc = preprocessing.LabelEncoder()\n",
    "    label_enc.fit(dataset_labels)  \n",
    "    dataset_labels_encoded = label_enc.transform(dataset_labels) # trasforma le label da caratteri a numeri\n",
    "    n_classes = len(list(label_enc.classes_))\n",
    "\n",
    "    # trasformo da list a np array\n",
    "    dataset_images = np.asarray(dataset_images)\n",
    "\n",
    "    # Aggiungo una extra-dimensione alle immagini e hot-encoding dei labels perche' e' come le vuole in input il modello cnn\n",
    "    #dataset_images_oland, dataset_labels_oland_encoded = add_extra_dim(dataset_images_oland, dataset_labels_oland_encoded, n_classes)\n",
    "    #dataset_images, dataset_labels_encoded = add_extra_dim(dataset_images, dataset_labels_encoded, n_classes)\n",
    "\n",
    "\n",
    "    #DATA AUGMENTATION\n",
    "    datagen = ImageDataGenerator(\n",
    "        rotation_range=2, # rotation\n",
    "        width_shift_range=0.1, # horizontal shift\n",
    "        height_shift_range=0.1, # vertical shift\n",
    "        zoom_range=0.05, # zoom\n",
    "        horizontal_flip=False, # horizontal flip\n",
    "        brightness_range=[0.2,1.2]) # brightness\n",
    "    #datagen.fit(X_train)\n",
    "\n",
    "\n",
    "    dataset_images_aug, dataset_labels_aug = [],[]\n",
    "    all_labels = list(set(dataset_labels))\n",
    "    for index_l,l in enumerate(all_labels):\n",
    "        img_one_class = []\n",
    "        for i,label in enumerate(dataset_labels):\n",
    "            if label == l:\n",
    "                img_one_class.append(dataset_images[i])\n",
    "        img_one_class = np.asarray(img_one_class)\n",
    "        label_one_class = [l for i in range(img_one_class.shape[0])]\n",
    "        label_one_class = label_enc.transform(label_one_class)\n",
    "        label_one_class = np.asarray(label_one_class)\n",
    "        img_one_class, label_one_class = add_extra_dim(img_one_class, label_one_class, n_classes)\n",
    "   \n",
    "        n_aug = int(dataset_images.shape[0]/(img_one_class.shape[0] * 10))\n",
    "        print(label_enc.inverse_transform(np.argmax(label_one_class, axis=1))[0], img_one_class.shape[0], n_aug)\n",
    "        img_one_class, label_one_class = data_augmentation(datagen, img_one_class, label_one_class, n_aug)\n",
    "        #print(imgs.shape)\n",
    "        #img_one_class = np.append(img_one_class, imgs, axis= 0)\n",
    "        #label_one_class = np.append(label_one_class, labs, axis= 0)\n",
    "\n",
    "        if index_l == 0:\n",
    "            dataset_images_aug = img_one_class\n",
    "            dataset_labels_aug = label_one_class\n",
    "        else:\n",
    "            dataset_images_aug = np.append(dataset_images_aug, img_one_class, axis= 0)\n",
    "            dataset_labels_aug = np.append(dataset_labels_aug, label_one_class, axis= 0)\n",
    "\n",
    "    if save==True:\n",
    "        save_dataset(path_to_save_aug, dataset_images_aug, categorical_to_decoded(dataset_labels_aug, label_enc))\n",
    "    \n",
    "    return dataset_images_aug, dataset_labels_aug, label_enc\n",
    "\n",
    "  #for i,img in enumerate(imgs):\n",
    "  #    plt.figure(i)\n",
    "  #    plt.imshow(imgs[i], cmap=\"gray\")\n",
    "  #plt.show()\n",
    "\n",
    "X_train_aug, y_train_aug, label_enc = create_and_save_data_augmentation_dataset(save=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8pAESdj0PWGb"
   },
   "outputs": [],
   "source": [
    "#DATA AUGMENTATION\n",
    "\n",
    "def aug_dataset(X, y, label_enc, path_to_save, save=False):\n",
    "\n",
    "    if X.ndim == 4:\n",
    "        X  = X.squeeze(axis=-1)\n",
    "    if y.ndim == 2:\n",
    "        y = categorical_to_decoded(y, label_enc)\n",
    "    \n",
    "    datagen = ImageDataGenerator(\n",
    "        rotation_range=2, # rotation\n",
    "        width_shift_range=0.1, # horizontal shift\n",
    "        height_shift_range=0.1, # vertical shift\n",
    "        zoom_range=0.05, # zoom\n",
    "        horizontal_flip=False, # horizontal flip\n",
    "        brightness_range=[0.2,1.2]) # brightness\n",
    "    #datagen.fit(X_train)\n",
    "\n",
    "    # Set dataset to increse\n",
    "    #X, y = X_train_flip, y_train_flip\n",
    "    \n",
    "    \n",
    "    X_aug, y_aug = [],[]\n",
    "    all_labels = list(set(y))\n",
    "    for index_l,l in enumerate(all_labels):\n",
    "        img_one_class = []\n",
    "        for i,label in enumerate(y):\n",
    "            if label == l:\n",
    "                img_one_class.append(X[i])\n",
    "        img_one_class = np.asarray(img_one_class)\n",
    "        label_one_class = [l for i in range(img_one_class.shape[0])]\n",
    "        label_one_class = label_enc.transform(label_one_class)\n",
    "        label_one_class = np.asarray(label_one_class)\n",
    "        img_one_class, label_one_class = add_extra_dim(img_one_class, label_one_class, n_classes)\n",
    "   \n",
    "        n_aug = int(X.shape[0]/(img_one_class.shape[0] * 10))\n",
    "        print(label_enc.inverse_transform(np.argmax(label_one_class, axis=1))[0], img_one_class.shape[0], n_aug)\n",
    "        img_one_class, label_one_class = data_augmentation(datagen, img_one_class, label_one_class, n_aug)\n",
    "        #print(imgs.shape)\n",
    "        #img_one_class = np.append(img_one_class, imgs, axis= 0)\n",
    "        #label_one_class = np.append(label_one_class, labs, axis= 0)\n",
    "\n",
    "        if index_l == 0:\n",
    "            X_aug = img_one_class\n",
    "            y_aug = label_one_class\n",
    "        else:\n",
    "            X_aug = np.append(X_aug, img_one_class, axis= 0)\n",
    "            y_aug = np.append(y_aug, label_one_class, axis= 0)\n",
    "\n",
    "    if save==True:\n",
    "        save_dataset(path_to_save, X_aug, categorical_to_decoded(y_aug, label_enc))\n",
    "    \n",
    "    return X_aug, y_aug\n",
    "\n",
    "  #for i,img in enumerate(imgs):\n",
    "  #    plt.figure(i)\n",
    "  #    plt.imshow(imgs[i], cmap=\"gray\")\n",
    "  #plt.show()\n",
    "\n",
    "X_aug, y_aug = aug_dataset(X_train_flip, y_train_flip, label_enc, save=True, path_to_save=\"/Users/marco/Desktop/train_flip_aug\")\n",
    "X2_aug, y2_aug = aug_dataset(X_test_flip, y_test_flip, label_enc, save=True, path_to_save=\"/Users/marco/Desktop/test_flip_aug\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9gehyT3LEGsz"
   },
   "outputs": [],
   "source": [
    "print('Dimensione array di immagini dopo data aug:',np.shape(X_test_flip))\n",
    "print('Dimensione array labels dopo il data aug:',np.shape(y_test_flip))  \n",
    "get_labels_number_in_category(y_test_flip, label_enc, view=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O8VP4h0zdw6Q"
   },
   "source": [
    "# K-fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SnRW_dlGd9wF",
    "outputId": "9b8a86d9-7d3f-4518-d45a-e24208a0f0f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset created\n",
      "dataset created\n",
      "dataset created\n",
      "dataset created\n",
      "1 (3447, 200, 200, 1) (862, 200, 200, 1)\n",
      "1 (3447, 40) (862, 40)\n",
      "dataset created\n",
      "dataset created\n",
      "dataset created\n",
      "dataset created\n",
      "2 (3447, 200, 200, 1) (862, 200, 200, 1)\n",
      "2 (3447, 40) (862, 40)\n",
      "dataset created\n",
      "dataset created\n",
      "dataset created\n",
      "dataset created\n",
      "3 (3447, 200, 200, 1) (862, 200, 200, 1)\n",
      "3 (3447, 40) (862, 40)\n",
      "dataset created\n",
      "dataset created\n",
      "dataset created\n",
      "dataset created\n",
      "4 (3447, 200, 200, 1) (862, 200, 200, 1)\n",
      "4 (3447, 40) (862, 40)\n",
      "dataset created\n",
      "dataset created\n",
      "dataset created\n",
      "dataset created\n",
      "5 (3448, 200, 200, 1) (861, 200, 200, 1)\n",
      "5 (3448, 40) (861, 40)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "\n",
    "count = 1\n",
    "for train_index, test_index in skf.split(X_tot, categorical_to_decoded(y_tot, label_enc)):\n",
    "    #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = X_tot[train_index], X_tot[test_index]\n",
    "    y_train, y_test = y_tot[train_index], y_tot[test_index]\n",
    "    \n",
    "    save_dataset(\"/Users/marco/Desktop/K-fold/\"+str(count)+\"/train\", X_train, categorical_to_decoded(y_train, label_enc))\n",
    "    save_dataset(\"/Users/marco/Desktop/K-fold/\"+str(count)+\"/train_HF\", X_train, categorical_to_decoded(y_train, label_enc), horizontal_flip=True)\n",
    "    save_dataset(\"/Users/marco/Desktop/K-fold/\"+str(count)+\"/test\", X_test, categorical_to_decoded(y_test, label_enc))\n",
    "    save_dataset(\"/Users/marco/Desktop/K-fold/\"+str(count)+\"/test_HF\", X_test, categorical_to_decoded(y_test, label_enc), horizontal_flip=True)\n",
    "    \n",
    "    print(str(count), X_train.shape, X_test.shape)\n",
    "    print(str(count), y_train.shape, y_test.shape)\n",
    "    count = count + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sxuopOoGNYWJ"
   },
   "source": [
    "# Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bHSDPK4G8Usy"
   },
   "outputs": [],
   "source": [
    "# MODEL WITH TRANSFER LEARNING\n",
    "base_model = tf.keras.applications.Xception(include_top=False, weights=\"imagenet\", \n",
    "                                    input_shape=(np.shape(X_train)[1], np.shape(X_train)[2], 3)) \n",
    "#model.summary()\n",
    "base_model.trainable = False\n",
    "inputs = keras.Input(shape=(np.shape(X_train)[1], np.shape(X_train)[2], 3))\n",
    "# We make sure that the base_model is running in inference mode here,\n",
    "# by passing `training=False`. This is important for fine-tuning, as you will\n",
    "# learn in a few paragraphs.\n",
    "x = base_model(inputs, training=False)\n",
    "# Convert features of shape `base_model.output_shape[1:]` to vectors\n",
    "x = keras.layers.GlobalAveragePooling2D()(x)\n",
    "# A Dense classifier with a single unit (binary classification)\n",
    "outputs = keras.layers.Dense(n_classes)(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.summary()\n",
    "\n",
    "# COMPILE\n",
    "model.compile(optimizer=Adam(0.001), \n",
    "            loss='categorical_crossentropy', \n",
    "            metrics=['accuracy'])\n",
    "\n",
    "# IMAGES GRAYSCALE TO \"RGB\"\n",
    "X_train = np.repeat(X_train, 3, -1)\n",
    "X_test_A = np.repeat(X_test_A, 3, -1)\n",
    "X_test_B = np.repeat(X_test_B, 3, -1)\n",
    "\n",
    "\n",
    "# NETWORK TRAINING\n",
    "#checkpoint = ModelCheckpoint(checkpoint_filepath + \"/weights.{epoch:02d}-{val_accuracy:.2f}.hdf5\" ,\n",
    "#                             save_weights_only=True,\n",
    "#                             monitor='val_accuracy',\n",
    "#                             mode='max',\n",
    "#                             save_best_only=True,\n",
    "#                             verbose=1,\n",
    "#                             save_freq='epoch')\n",
    "#history = model.fit(X_train, y_train,\n",
    "#                    validation_split=0.17, \n",
    "#                    epochs=300, callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1433,
     "status": "ok",
     "timestamp": 1615308568346,
     "user": {
      "displayName": "Marco Loschiavo",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi9s3_7KCzffUmLttlgv1bC9AxkHEpGTlOL7vWRvA=s64",
      "userId": "16918725666776522759"
     },
     "user_tz": -60
    },
    "id": "4zz8lvJC-PTJ",
    "outputId": "abfd6368-0f7e-4ec6-f2b4-ec5de4bb65e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"my_xception3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 200, 200, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv1 (Conv2D)           (None, 99, 99, 32)   288         input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv1_bn (BatchNormaliza (None, 99, 99, 32)   128         block1_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv1_act (Activation)   (None, 99, 99, 32)   0           block1_conv1_bn[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv2 (Conv2D)           (None, 97, 97, 64)   18432       block1_conv1_act[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv2_bn (BatchNormaliza (None, 97, 97, 64)   256         block1_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv2_act (Activation)   (None, 97, 97, 64)   0           block1_conv2_bn[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block2_sepconv1 (SeparableConv2 (None, 97, 97, 128)  8768        block1_conv2_act[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block2_sepconv1_bn (BatchNormal (None, 97, 97, 128)  512         block2_sepconv1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block2_sepconv2_act (Activation (None, 97, 97, 128)  0           block2_sepconv1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block2_sepconv2 (SeparableConv2 (None, 97, 97, 128)  17536       block2_sepconv2_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block2_sepconv2_bn (BatchNormal (None, 97, 97, 128)  512         block2_sepconv2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 49, 49, 128)  8192        block1_conv2_act[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block2_pool (MaxPooling2D)      (None, 49, 49, 128)  0           block2_sepconv2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 49, 49, 128)  512         conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_12 (Add)                    (None, 49, 49, 128)  0           block2_pool[0][0]                \n",
      "                                                                 batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block3_sepconv1_act (Activation (None, 49, 49, 128)  0           add_12[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block3_sepconv1 (SeparableConv2 (None, 49, 49, 256)  33920       block3_sepconv1_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block3_sepconv1_bn (BatchNormal (None, 49, 49, 256)  1024        block3_sepconv1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block3_sepconv2_act (Activation (None, 49, 49, 256)  0           block3_sepconv1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block3_sepconv2 (SeparableConv2 (None, 49, 49, 256)  67840       block3_sepconv2_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block3_sepconv2_bn (BatchNormal (None, 49, 49, 256)  1024        block3_sepconv2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 25, 25, 256)  32768       add_12[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block3_pool (MaxPooling2D)      (None, 25, 25, 256)  0           block3_sepconv2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 25, 25, 256)  1024        conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_13 (Add)                    (None, 25, 25, 256)  0           block3_pool[0][0]                \n",
      "                                                                 batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block4_sepconv1_act (Activation (None, 25, 25, 256)  0           add_13[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block4_sepconv1 (SeparableConv2 (None, 25, 25, 512)  133376      block4_sepconv1_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block4_sepconv1_bn (BatchNormal (None, 25, 25, 512)  2048        block4_sepconv1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block4_sepconv2_act (Activation (None, 25, 25, 512)  0           block4_sepconv1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block4_sepconv2 (SeparableConv2 (None, 25, 25, 512)  266752      block4_sepconv2_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block4_sepconv2_bn (BatchNormal (None, 25, 25, 512)  2048        block4_sepconv2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 13, 13, 512)  131072      add_13[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block4_pool (MaxPooling2D)      (None, 13, 13, 512)  0           block4_sepconv2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 13, 13, 512)  2048        conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_14 (Add)                    (None, 13, 13, 512)  0           block4_pool[0][0]                \n",
      "                                                                 batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv1_act (Activation (None, 13, 13, 512)  0           add_14[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv1 (SeparableConv2 (None, 13, 13, 512)  266752      block5_sepconv1_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv1_bn (BatchNormal (None, 13, 13, 512)  2048        block5_sepconv1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv2_act (Activation (None, 13, 13, 512)  0           block5_sepconv1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv2 (SeparableConv2 (None, 13, 13, 512)  266752      block5_sepconv2_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv2_bn (BatchNormal (None, 13, 13, 512)  2048        block5_sepconv2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv3_act (Activation (None, 13, 13, 512)  0           block5_sepconv2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv3 (SeparableConv2 (None, 13, 13, 512)  266752      block5_sepconv3_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv3_bn (BatchNormal (None, 13, 13, 512)  2048        block5_sepconv3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_15 (Add)                    (None, 13, 13, 512)  0           block5_sepconv3_bn[0][0]         \n",
      "                                                                 add_14[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv1_act (Activation (None, 13, 13, 512)  0           add_15[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv1 (SeparableConv2 (None, 13, 13, 512)  266752      block6_sepconv1_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv1_bn (BatchNormal (None, 13, 13, 512)  2048        block6_sepconv1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv2_act (Activation (None, 13, 13, 512)  0           block6_sepconv1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv2 (SeparableConv2 (None, 13, 13, 512)  266752      block6_sepconv2_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv2_bn (BatchNormal (None, 13, 13, 512)  2048        block6_sepconv2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv3_act (Activation (None, 13, 13, 512)  0           block6_sepconv2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv3 (SeparableConv2 (None, 13, 13, 512)  266752      block6_sepconv3_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv3_bn (BatchNormal (None, 13, 13, 512)  2048        block6_sepconv3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_16 (Add)                    (None, 13, 13, 512)  0           block6_sepconv3_bn[0][0]         \n",
      "                                                                 add_15[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv1_act (Activation (None, 13, 13, 512)  0           add_16[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv1 (SeparableConv2 (None, 13, 13, 512)  266752      block7_sepconv1_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv1_bn (BatchNormal (None, 13, 13, 512)  2048        block7_sepconv1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv2_act (Activation (None, 13, 13, 512)  0           block7_sepconv1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv2 (SeparableConv2 (None, 13, 13, 512)  266752      block7_sepconv2_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv2_bn (BatchNormal (None, 13, 13, 512)  2048        block7_sepconv2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv3_act (Activation (None, 13, 13, 512)  0           block7_sepconv2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv3 (SeparableConv2 (None, 13, 13, 512)  266752      block7_sepconv3_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv3_bn (BatchNormal (None, 13, 13, 512)  2048        block7_sepconv3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_17 (Add)                    (None, 13, 13, 512)  0           block7_sepconv3_bn[0][0]         \n",
      "                                                                 add_16[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv1_act (Activation (None, 13, 13, 512)  0           add_17[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv1 (SeparableConv2 (None, 13, 13, 512)  266752      block8_sepconv1_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv1_bn (BatchNormal (None, 13, 13, 512)  2048        block8_sepconv1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv2_act (Activation (None, 13, 13, 512)  0           block8_sepconv1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv2 (SeparableConv2 (None, 13, 13, 512)  266752      block8_sepconv2_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv2_bn (BatchNormal (None, 13, 13, 512)  2048        block8_sepconv2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv3_act (Activation (None, 13, 13, 512)  0           block8_sepconv2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv3 (SeparableConv2 (None, 13, 13, 512)  266752      block8_sepconv3_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv3_bn (BatchNormal (None, 13, 13, 512)  2048        block8_sepconv3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_18 (Add)                    (None, 13, 13, 512)  0           block8_sepconv3_bn[0][0]         \n",
      "                                                                 add_17[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block13_sepconv1_act (Activatio (None, 13, 13, 512)  0           add_18[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block13_sepconv1 (SeparableConv (None, 13, 13, 512)  266752      block13_sepconv1_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block13_sepconv1_bn (BatchNorma (None, 13, 13, 512)  2048        block13_sepconv1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block13_sepconv2_act (Activatio (None, 13, 13, 512)  0           block13_sepconv1_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block13_sepconv2 (SeparableConv (None, 13, 13, 512)  266752      block13_sepconv2_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block13_sepconv2_bn (BatchNorma (None, 13, 13, 512)  2048        block13_sepconv2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 7, 7, 512)    262144      add_18[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block13_pool (MaxPooling2D)     (None, 7, 7, 512)    0           block13_sepconv2_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 7, 7, 512)    2048        conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_19 (Add)                    (None, 7, 7, 512)    0           block13_pool[0][0]               \n",
      "                                                                 batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block14_sepconv1 (SeparableConv (None, 7, 7, 728)    377344      add_19[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block14_sepconv1_bn (BatchNorma (None, 7, 7, 728)    2912        block14_sepconv1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block14_sepconv1_act (Activatio (None, 7, 7, 728)    0           block14_sepconv1_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "avg_pool (GlobalAveragePooling2 (None, 728)          0           block14_sepconv1_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "predictions (Dense)             (None, 40)           29160       avg_pool[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 5,166,888\n",
      "Trainable params: 5,144,504\n",
      "Non-trainable params: 22,384\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# MODEL\n",
    "model = my_Xception3(shape=(np.shape(X_train)[1], np.shape(X_train)[2], 1), n_classes=n_classes)\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer=Adam(0.001), \n",
    "              loss='categorical_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#model.compile(optimizer=SGD(lr=0.01, momentum=0.9), \n",
    "#                  loss='categorical_crossentropy', \n",
    "#                  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 2880720,
     "status": "error",
     "timestamp": 1615311461494,
     "user": {
      "displayName": "Marco Loschiavo",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi9s3_7KCzffUmLttlgv1bC9AxkHEpGTlOL7vWRvA=s64",
      "userId": "16918725666776522759"
     },
     "user_tz": -60
    },
    "id": "gFuR9er9cQYd",
    "outputId": "a1228d48-f393-428c-a826-e3a8708e1a3f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2021-03-09 16:49:41: current learning rate is 0.00100000\n",
      "190/190 [==============================] - 138s 705ms/step - loss: 1.9801 - accuracy: 0.4754 - val_loss: 5.1880 - val_accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 00001: saving model to /content/drive/MyDrive/ColabNotebooks/weights/weights.01-0.00.hdf5\n",
      "Epoch 2/100\n",
      "2021-03-09 16:52:00: current learning rate is 0.00100000\n",
      "190/190 [==============================] - 129s 682ms/step - loss: 0.5375 - accuracy: 0.8458 - val_loss: 9.0867 - val_accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 00002: saving model to /content/drive/MyDrive/ColabNotebooks/weights/weights.02-0.00.hdf5\n",
      "Epoch 3/100\n",
      "2021-03-09 16:54:10: current learning rate is 0.00100000\n",
      "190/190 [==============================] - 130s 682ms/step - loss: 0.2506 - accuracy: 0.9289 - val_loss: 12.0491 - val_accuracy: 0.0932\n",
      "\n",
      "Epoch 00003: saving model to /content/drive/MyDrive/ColabNotebooks/weights/weights.03-0.09.hdf5\n",
      "Epoch 4/100\n",
      "2021-03-09 16:56:20: current learning rate is 0.00100000\n",
      "190/190 [==============================] - 129s 679ms/step - loss: 0.1348 - accuracy: 0.9591 - val_loss: 13.6074 - val_accuracy: 0.1020\n",
      "\n",
      "Epoch 00004: saving model to /content/drive/MyDrive/ColabNotebooks/weights/weights.04-0.10.hdf5\n",
      "Epoch 5/100\n",
      "2021-03-09 16:58:30: current learning rate is 0.00100000\n",
      "190/190 [==============================] - 130s 682ms/step - loss: 0.0821 - accuracy: 0.9746 - val_loss: 17.1560 - val_accuracy: 0.0916\n",
      "\n",
      "Epoch 00005: saving model to /content/drive/MyDrive/ColabNotebooks/weights/weights.05-0.09.hdf5\n",
      "Epoch 6/100\n",
      "2021-03-09 17:00:40: current learning rate is 0.00100000\n",
      "190/190 [==============================] - 129s 680ms/step - loss: 0.0956 - accuracy: 0.9715 - val_loss: 15.8168 - val_accuracy: 0.1092\n",
      "\n",
      "Epoch 00006: saving model to /content/drive/MyDrive/ColabNotebooks/weights/weights.06-0.11.hdf5\n",
      "Epoch 7/100\n",
      "2021-03-09 17:02:50: current learning rate is 0.00100000\n",
      "190/190 [==============================] - 130s 682ms/step - loss: 0.0363 - accuracy: 0.9920 - val_loss: 16.9428 - val_accuracy: 0.1036\n",
      "\n",
      "Epoch 00007: saving model to /content/drive/MyDrive/ColabNotebooks/weights/weights.07-0.10.hdf5\n",
      "Epoch 8/100\n",
      "2021-03-09 17:05:01: current learning rate is 0.00100000\n",
      "190/190 [==============================] - 130s 682ms/step - loss: 0.0508 - accuracy: 0.9855 - val_loss: 18.0602 - val_accuracy: 0.0570\n",
      "\n",
      "Epoch 00008: saving model to /content/drive/MyDrive/ColabNotebooks/weights/weights.08-0.06.hdf5\n",
      "Epoch 9/100\n",
      "2021-03-09 17:07:11: current learning rate is 0.00100000\n",
      "190/190 [==============================] - 129s 680ms/step - loss: 0.0218 - accuracy: 0.9946 - val_loss: 12.2209 - val_accuracy: 0.1060\n",
      "\n",
      "Epoch 00009: saving model to /content/drive/MyDrive/ColabNotebooks/weights/weights.09-0.11.hdf5\n",
      "Epoch 10/100\n",
      "2021-03-09 17:09:21: current learning rate is 0.00100000\n",
      "190/190 [==============================] - 128s 676ms/step - loss: 0.0326 - accuracy: 0.9928 - val_loss: 19.7785 - val_accuracy: 0.0827\n",
      "\n",
      "Epoch 00010: saving model to /content/drive/MyDrive/ColabNotebooks/weights/weights.10-0.08.hdf5\n",
      "Epoch 11/100\n",
      "2021-03-09 17:11:30: current learning rate is 0.00100000\n",
      "190/190 [==============================] - 129s 678ms/step - loss: 0.0508 - accuracy: 0.9863 - val_loss: 13.8411 - val_accuracy: 0.1036\n",
      "\n",
      "Epoch 00011: saving model to /content/drive/MyDrive/ColabNotebooks/weights/weights.11-0.10.hdf5\n",
      "Epoch 12/100\n",
      "2021-03-09 17:13:40: current learning rate is 0.00100000\n",
      "190/190 [==============================] - 129s 678ms/step - loss: 0.0316 - accuracy: 0.9902 - val_loss: 15.7829 - val_accuracy: 0.1052\n",
      "\n",
      "Epoch 00012: saving model to /content/drive/MyDrive/ColabNotebooks/weights/weights.12-0.11.hdf5\n",
      "Epoch 13/100\n",
      "2021-03-09 17:15:50: current learning rate is 0.00100000\n",
      "190/190 [==============================] - 129s 679ms/step - loss: 0.0350 - accuracy: 0.9929 - val_loss: 14.5336 - val_accuracy: 0.1100\n",
      "\n",
      "Epoch 00013: saving model to /content/drive/MyDrive/ColabNotebooks/weights/weights.13-0.11.hdf5\n",
      "Epoch 14/100\n",
      "2021-03-09 17:18:00: current learning rate is 0.00100000\n",
      "190/190 [==============================] - 129s 679ms/step - loss: 0.0402 - accuracy: 0.9868 - val_loss: 15.1361 - val_accuracy: 0.1036\n",
      "\n",
      "Epoch 00014: saving model to /content/drive/MyDrive/ColabNotebooks/weights/weights.14-0.10.hdf5\n",
      "Epoch 15/100\n",
      "2021-03-09 17:20:09: current learning rate is 0.00100000\n",
      "190/190 [==============================] - 128s 676ms/step - loss: 0.0241 - accuracy: 0.9938 - val_loss: 12.4786 - val_accuracy: 0.1036\n",
      "\n",
      "Epoch 00015: saving model to /content/drive/MyDrive/ColabNotebooks/weights/weights.15-0.10.hdf5\n",
      "Epoch 16/100\n",
      "2021-03-09 17:22:19: current learning rate is 0.00100000\n",
      "190/190 [==============================] - 130s 684ms/step - loss: 0.0248 - accuracy: 0.9937 - val_loss: 14.7813 - val_accuracy: 0.1076\n",
      "\n",
      "Epoch 00016: saving model to /content/drive/MyDrive/ColabNotebooks/weights/weights.16-0.11.hdf5\n",
      "Epoch 17/100\n",
      "2021-03-09 17:24:29: current learning rate is 0.00100000\n",
      "190/190 [==============================] - 130s 686ms/step - loss: 0.0071 - accuracy: 0.9985 - val_loss: 13.9476 - val_accuracy: 0.1076\n",
      "\n",
      "Epoch 00017: saving model to /content/drive/MyDrive/ColabNotebooks/weights/weights.17-0.11.hdf5\n",
      "Epoch 18/100\n",
      "2021-03-09 17:26:41: current learning rate is 0.00100000\n",
      "190/190 [==============================] - 129s 681ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 14.3280 - val_accuracy: 0.1092\n",
      "\n",
      "Epoch 00018: saving model to /content/drive/MyDrive/ColabNotebooks/weights/weights.18-0.11.hdf5\n",
      "Epoch 19/100\n",
      "2021-03-09 17:28:51: current learning rate is 0.00100000\n",
      "190/190 [==============================] - 130s 683ms/step - loss: 2.5949e-04 - accuracy: 1.0000 - val_loss: 14.0168 - val_accuracy: 0.1084\n",
      "\n",
      "Epoch 00019: saving model to /content/drive/MyDrive/ColabNotebooks/weights/weights.19-0.11.hdf5\n",
      "Epoch 20/100\n",
      "2021-03-09 17:31:01: current learning rate is 0.00100000\n",
      "190/190 [==============================] - 130s 684ms/step - loss: 3.8059e-04 - accuracy: 1.0000 - val_loss: 14.2316 - val_accuracy: 0.1092\n",
      "\n",
      "Epoch 00020: saving model to /content/drive/MyDrive/ColabNotebooks/weights/weights.20-0.11.hdf5\n",
      "Epoch 21/100\n",
      "2021-03-09 17:33:12: current learning rate is 0.00100000\n",
      "190/190 [==============================] - 130s 682ms/step - loss: 1.8631e-04 - accuracy: 1.0000 - val_loss: 14.4186 - val_accuracy: 0.1084\n",
      "\n",
      "Epoch 00021: saving model to /content/drive/MyDrive/ColabNotebooks/weights/weights.21-0.11.hdf5\n",
      "Epoch 22/100\n",
      "2021-03-09 17:35:22: current learning rate is 0.00100000\n",
      "190/190 [==============================] - 129s 677ms/step - loss: 1.3497e-04 - accuracy: 1.0000 - val_loss: 14.6332 - val_accuracy: 0.1084\n",
      "\n",
      "Epoch 00022: saving model to /content/drive/MyDrive/ColabNotebooks/weights/weights.22-0.11.hdf5\n",
      "Epoch 23/100\n",
      "2021-03-09 17:37:32: current learning rate is 0.00100000\n",
      " 13/190 [=>............................] - ETA: 1:57 - loss: 9.0331e-05 - accuracy: 1.0000"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-41e72fbcbc4d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m history = model.fit(X_train_flip, y_train_flip,\n\u001b[1;32m     16\u001b[0m                     \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.17\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m                     epochs=100, shuffle=True, callbacks=[checkpoint, lr_scheduler, tensorboard_callback])\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2943\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2945\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1919\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# NETWORK TRAINING\n",
    "log_dir = log_filepath + \"/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "checkpoint = ModelCheckpoint(checkpoint_filepath + \"/weights.{epoch:02d}-{val_accuracy:.2f}.hdf5\" ,\n",
    "                             save_weights_only=True,\n",
    "                             monitor='val_accuracy',\n",
    "                             mode='max',\n",
    "                             save_best_only=False,\n",
    "                             verbose=1,\n",
    "                             save_freq='epoch')\n",
    "\n",
    "#csv_logger = CSVLogger(log_filepath + '/log.csv', append=True, separator=',')\n",
    "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
    "history = model.fit(X_train_flip, y_train_flip,\n",
    "                    validation_split=0.17, \n",
    "                    epochs=100, shuffle=True, callbacks=[checkpoint, lr_scheduler, tensorboard_callback])\n",
    "\n",
    "from google.colab import files\n",
    "np.save('/content/drive/MyDrive/ColabNotebooks/my_history.npy',history.history)\n",
    "files.download('/content/drive/MyDrive/ColabNotebooks/my_history.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lL2coJiydlJ7"
   },
   "outputs": [],
   "source": [
    "%tensorboard --logdir /content/drive/MyDrive/ColabNotebooks/log/fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rgH9JokENYWK"
   },
   "outputs": [],
   "source": [
    "# EVALUATE\n",
    "loss, accuracy = model.evaluate(X_test_B, y_test_B, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XA_Q1GWpuEDb"
   },
   "outputs": [],
   "source": [
    "# CHOOSE THE BEST WEIGHTS\n",
    "X_test_tot, y_test_tot = np.append(X_test_A, X_test_B, axis= 0), np.append(y_test_A, y_test_B, axis= 0)\n",
    "\n",
    "accuracy_max = 0\n",
    "weights_max = \"\"\n",
    "for weights in os.listdir(weights_path):\n",
    "    if(weights.endswith(\".hdf5\")):\n",
    "        print(weights)\n",
    "        model.load_weights(weights_path +\"/\"+ weights)\n",
    "        lossA, accuracyA = model.evaluate(X_test_A, y_test_A, verbose=1)\n",
    "        lossB, accuracyB = model.evaluate(X_test_B, y_test_B, verbose=1)\n",
    "        loss, accuracy = model.evaluate(X_test_tot, y_test_tot, verbose=1)\n",
    "        if accuracy_max < accuracy:\n",
    "            accuracy_max = accuracy\n",
    "            weights_max = weights\n",
    "        print(\"---------------------------------------------------------------------------------------------\")\n",
    "print(weights_max + \"  accuracy: \" + str(accuracy_max))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QPzb7zleNYWL"
   },
   "outputs": [],
   "source": [
    "# LOAD AND SAVE WEIGHTS AND HISTORY\n",
    "#model.save_weights(\"/content/checkpoint/weights.34-0.71.hdf5\")\n",
    "\n",
    "#model.load_weights(\"/content/checkpoint/weights.02-0.95.hdf5\")\n",
    "\n",
    "#np.save('/content/drive/MyDrive/ColabNotebooks/my_history.npy',history.history)\n",
    "#history2 = np.load('/content/drive/MyDrive/ColabNotebooks/my_history.npy', allow_pickle='TRUE').item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bJ95vct4NYWL"
   },
   "outputs": [],
   "source": [
    "# PLOT TRAINING AND VALIDATION ACCURACY\n",
    "plot_train_validation_loss_accuracy2(history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "scu1rM84NYWM"
   },
   "outputs": [],
   "source": [
    "# PREDICTION\n",
    "#probability_model = tf.keras.Sequential([model, tf.keras.layers.Softmax()])\n",
    "#predictions = probability_model.predict(X_test_A)\n",
    "predictions = model.predict(X_test_tot)\n",
    "\n",
    "get_prediction_data(predictions, X_test_tot, y_test_tot, label_enc, \n",
    "                    summary=False, details=True, plot=(None,\"all\",\"V30\"), y_train=y_train)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "O8VP4h0zdw6Q"
   ],
   "name": "classification_VFinal.ipynb",
   "provenance": [
    {
     "file_id": "1IzRZjyQ-Hg8zHJ4COzUXl7HwQdNY0NlX",
     "timestamp": 1611148825127
    },
    {
     "file_id": "1YGK1trHqSwnbprRNfct2uGhqGLd3WEOv",
     "timestamp": 1610610702581
    },
    {
     "file_id": "1tjCTo2xwtSzOLdCK03UVxha8Z6zGDwhK",
     "timestamp": 1610610656662
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
